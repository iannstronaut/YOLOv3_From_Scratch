{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUOp7XYYvQsn40vQHRulkT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iannstronaut/YOLOv3_From_Scratch/blob/main/Build_Yolo_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Implementation"
      ],
      "metadata": {
        "id": "htrDXOncw_d0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-V0nbusGsXgj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = [\n",
        "    (32, 3, 1),\n",
        "    (64, 3, 2),\n",
        "    [\"B\", 1],\n",
        "    (128, 3, 2),\n",
        "    [\"B\", 2],\n",
        "    (256, 3, 2),\n",
        "    [\"B\", 8],\n",
        "    (512, 3, 2),\n",
        "    [\"B\", 8],\n",
        "    (1024, 3, 2),\n",
        "    [\"B\", 4], # To this point is Darknet-53\n",
        "    (512, 1, 1),\n",
        "    (1024, 3, 1),\n",
        "    \"S\",\n",
        "    (256, 1, 1),\n",
        "    \"U\",\n",
        "    (256, 1, 1),\n",
        "    (512, 3, 1),\n",
        "    \"S\",\n",
        "    (128, 1, 1),\n",
        "    \"U\",\n",
        "    (128, 1, 1),\n",
        "    (256, 3, 1),\n",
        "    \"S\"\n",
        "]"
      ],
      "metadata": {
        "id": "XSCnwtaBtAjU"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variabel model_config adalah sebuah daftar (list) yang berisi konfigurasi arsitektur dari model YOLOv3, dimulai dari Darknet-53 sebagai backbone, lalu dilanjutkan dengan bagian head untuk deteksi objek.\n",
        "\n",
        "Setiap elemen dalam list ini dapat berupa:\n",
        "\n",
        "* Tuple (C, K, S) yang menyatakan layer Convolution (Conv2D):\n",
        "\n",
        "  * C: Jumlah filter/output channels\n",
        "\n",
        "  * K: Ukuran kernel (kernel size)\n",
        "\n",
        "  * S: Langkah (stride)\n",
        "\n",
        "* List [\"B\", N] menyatakan blok residual sebanyak N kali (bagian dari Darknet-53).\n",
        "\n",
        "* String \"S\" menyatakan deteksi skala (scale detection).\n",
        "\n",
        "* String \"U\" menyatakan upsampling layer (untuk feature map fusion dari feature map sebelumnya)."
      ],
      "metadata": {
        "id": "Enllv7WJik3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
        "    self.bn = nn.BatchNorm2d(out_channels)\n",
        "    self.leaky = nn.LeakyReLU(0.1)\n",
        "    self.use_bn_act = bn_act\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.use_bn_act:\n",
        "      return self.leaky(self.bn(self.conv(x)))\n",
        "    else:\n",
        "      return self.conv(x)"
      ],
      "metadata": {
        "id": "-Tz5mb9fuXQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class CNNBlock adalah sebuah modul (class) dari PyTorch yang digunakan untuk membangun blok dasar dalam arsitektur jaringan konvolusional seperti YOLOv3. Blok ini menggabungkan beberapa operasi utama yang umum digunakan dalam CNN modern, yaitu Conv2D (konvolusi), Batch Normalization, dan Leaky ReLU dalam satu kesatuan.\n",
        "\n",
        "Blok ini memiliki parameter masukan in_channels dan out_channels yang masing-masing menentukan jumlah channel pada input dan output dari layer konvolusi. Parameter bn_act adalah boolean yang digunakan untuk menentukan apakah akan menggunakan Batch Normalization dan aktivasi Leaky ReLU setelah konvolusi atau tidak. Jika bn_act bernilai True, maka konvolusi akan dilanjutkan dengan proses normalisasi dan aktivasi, sedangkan jika False, hanya layer konvolusi saja yang digunakan (biasanya untuk layer output YOLO).\n",
        "\n",
        "Operasi dalam blok ini dilakukan sebagai berikut:\n",
        "\n",
        "* Jika bn_act = True, maka input akan melewati Conv2D → BatchNorm2D → LeakyReLU.\n",
        "\n",
        "* Jika bn_act = False, maka input hanya akan melewati Conv2D tanpa aktivasi atau normalisasi.\n",
        "\n",
        "Selain itu, bias pada layer konvolusi hanya diaktifkan jika bn_act = False. Hal ini karena jika menggunakan BatchNorm, nilai bias menjadi tidak relevan dan sebaiknya di-nonaktifkan untuk efisiensi.\n",
        "\n",
        "Dengan fleksibilitas seperti ini, CNNBlock dapat digunakan secara dinamis baik untuk bagian-bagian awal jaringan, blok intermediate, maupun layer akhir yang tidak memerlukan aktivasi tambahan."
      ],
      "metadata": {
        "id": "xqs4dUX4jKSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, channels, use_residual=True, num_repeats=1):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList()\n",
        "    for _ in range(num_repeats):\n",
        "      self.layers += [\n",
        "          nn.Sequential(\n",
        "            CNNBlock(channels, channels//2, kernel_size=1),\n",
        "            CNNBlock(channels//2, channels, kernel_size=3, padding=1)\n",
        "          )\n",
        "      ]\n",
        "\n",
        "    self.use_residual = use_residual\n",
        "    self.num_repeats = num_repeats\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x) + x if self.use_residual else layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "5DzZTEaqujyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScalePrediction(nn.Module):\n",
        "  def __init__(self, in_channels, num_classes):\n",
        "    super().__init__()\n",
        "    self.pred = nn.Sequential(\n",
        "        CNNBlock(in_channels, 2*in_channels, kernel_size=3, padding=1),\n",
        "        CNNBlock(2*in_channels, (num_classes + 5 ) * 3, bn_act=False, kernel_size=1)\n",
        "    )\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    return (\n",
        "        self.pred(x)\n",
        "        .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
        "        .permute(0, 1, 3, 4, 2)\n",
        "    )"
      ],
      "metadata": {
        "id": "E7ARfm0Lun35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLOv3(nn.Module):\n",
        "  def __init__(self, in_channels=3, num_classes=20):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.in_channels = in_channels\n",
        "    self.layers = self._create_conv_layers()\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = []\n",
        "    route_connections = []\n",
        "\n",
        "    for layer in self.layers:\n",
        "      if isinstance(layer, ScalePrediction):\n",
        "        output.append(layer(x))\n",
        "        continue\n",
        "\n",
        "      x = layer(x)\n",
        "      print(x.shape)\n",
        "\n",
        "      if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
        "        route_connections.append(x)\n",
        "\n",
        "      elif isinstance(layer, nn.Upsample):\n",
        "        x = torch.cat([x, route_connections[-1]], dim = 1)\n",
        "        route_connections.pop()\n",
        "\n",
        "    return output\n",
        "\n",
        "  def _create_conv_layers(self):\n",
        "    layers = nn.ModuleList()\n",
        "    in_channels = self.in_channels\n",
        "\n",
        "    for module in model_config:\n",
        "      if isinstance(module, tuple):\n",
        "        out_channels, kernel_size, stride = module\n",
        "        layers.append(CNNBlock(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=1 if kernel_size == 3 else 0\n",
        "          )\n",
        "        )\n",
        "        in_channels = out_channels\n",
        "\n",
        "      elif isinstance(module, list):\n",
        "        num_repeats = module[1]\n",
        "        layers.append(ResidualBlock(in_channels, num_repeats=num_repeats))\n",
        "\n",
        "      elif isinstance(module, str):\n",
        "        if module == \"S\":\n",
        "          layers += [\n",
        "              ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
        "              CNNBlock(in_channels, in_channels//2, kernel_size=1),\n",
        "              ScalePrediction(in_channels//2, num_classes=self.num_classes)\n",
        "          ]\n",
        "          in_channels = in_channels // 2\n",
        "\n",
        "        elif module == \"U\":\n",
        "          layers.append(nn.Upsample(scale_factor=2))\n",
        "          in_channels = in_channels * 3\n",
        "\n",
        "    return layers"
      ],
      "metadata": {
        "id": "yNFiL8-buq3_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  num_classes = 20\n",
        "  IMAGE_SIZE = 416\n",
        "  model = YOLOv3(num_classes=num_classes)\n",
        "  x = torch.randn((2, 3, IMAGE_SIZE, IMAGE_SIZE))\n",
        "  out = model(x)\n",
        "  assert model(x)[0].shape == (2, 3, IMAGE_SIZE//32, IMAGE_SIZE//32, num_classes + 5)\n",
        "  assert model(x)[1].shape == (2, 3, IMAGE_SIZE//16, IMAGE_SIZE//16, num_classes + 5)\n",
        "  assert model(x)[2].shape == (2, 3, IMAGE_SIZE//8, IMAGE_SIZE//8, num_classes + 5)\n",
        "  print(\"Success!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPCfVR_d1f9E",
        "outputId": "39e6b92c-ff28-418d-8633-8008ea6fe382"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 32, 416, 416])\n",
            "torch.Size([2, 64, 208, 208])\n",
            "torch.Size([2, 64, 208, 208])\n",
            "torch.Size([2, 128, 104, 104])\n",
            "torch.Size([2, 128, 104, 104])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 512, 13, 13])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 512, 13, 13])\n",
            "torch.Size([2, 256, 13, 13])\n",
            "torch.Size([2, 256, 26, 26])\n",
            "torch.Size([2, 256, 26, 26])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 256, 26, 26])\n",
            "torch.Size([2, 128, 26, 26])\n",
            "torch.Size([2, 128, 52, 52])\n",
            "torch.Size([2, 128, 52, 52])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 128, 52, 52])\n",
            "torch.Size([2, 32, 416, 416])\n",
            "torch.Size([2, 64, 208, 208])\n",
            "torch.Size([2, 64, 208, 208])\n",
            "torch.Size([2, 128, 104, 104])\n",
            "torch.Size([2, 128, 104, 104])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 512, 13, 13])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 512, 13, 13])\n",
            "torch.Size([2, 256, 13, 13])\n",
            "torch.Size([2, 256, 26, 26])\n",
            "torch.Size([2, 256, 26, 26])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 256, 26, 26])\n",
            "torch.Size([2, 128, 26, 26])\n",
            "torch.Size([2, 128, 52, 52])\n",
            "torch.Size([2, 128, 52, 52])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 128, 52, 52])\n",
            "torch.Size([2, 32, 416, 416])\n",
            "torch.Size([2, 64, 208, 208])\n",
            "torch.Size([2, 64, 208, 208])\n",
            "torch.Size([2, 128, 104, 104])\n",
            "torch.Size([2, 128, 104, 104])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 512, 13, 13])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 512, 13, 13])\n",
            "torch.Size([2, 256, 13, 13])\n",
            "torch.Size([2, 256, 26, 26])\n",
            "torch.Size([2, 256, 26, 26])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 256, 26, 26])\n",
            "torch.Size([2, 128, 26, 26])\n",
            "torch.Size([2, 128, 52, 52])\n",
            "torch.Size([2, 128, 52, 52])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 128, 52, 52])\n",
            "torch.Size([2, 32, 416, 416])\n",
            "torch.Size([2, 64, 208, 208])\n",
            "torch.Size([2, 64, 208, 208])\n",
            "torch.Size([2, 128, 104, 104])\n",
            "torch.Size([2, 128, 104, 104])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 512, 13, 13])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 1024, 13, 13])\n",
            "torch.Size([2, 512, 13, 13])\n",
            "torch.Size([2, 256, 13, 13])\n",
            "torch.Size([2, 256, 26, 26])\n",
            "torch.Size([2, 256, 26, 26])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 512, 26, 26])\n",
            "torch.Size([2, 256, 26, 26])\n",
            "torch.Size([2, 128, 26, 26])\n",
            "torch.Size([2, 128, 52, 52])\n",
            "torch.Size([2, 128, 52, 52])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 256, 52, 52])\n",
            "torch.Size([2, 128, 52, 52])\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Class"
      ],
      "metadata": {
        "id": "gACbUMvx40Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aladdinpersson/Machine-Learning-Collection.git\n",
        "%cd Machine-Learning-Collection/ML/Pytorch/object_detection/YOLOv3\n",
        "!cp utils.py /content/\n",
        "!cp config.py /content/\n",
        "!cp dataset.py /content/\n",
        "%cd /content/\n",
        "!rm -rf Machine-Learning-Collection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZa7cHfGFxbp",
        "outputId": "295e989e-d9de-4c7e-a541-9c1eba7c9732"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Machine-Learning-Collection'...\n",
            "remote: Enumerating objects: 1360, done.\u001b[K\n",
            "remote: Counting objects: 100% (335/335), done.\u001b[K\n",
            "remote: Compressing objects: 100% (200/200), done.\u001b[K\n",
            "remote: Total 1360 (delta 172), reused 135 (delta 135), pack-reused 1025 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1360/1360), 120.81 MiB | 30.61 MiB/s, done.\n",
            "Resolving deltas: 100% (565/565), done.\n",
            "/content/Machine-Learning-Collection/ML/Pytorch/object_detection/YOLOv3\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import config\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image, ImageFile\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from utils import (\n",
        "    iou_width_height as iou,\n",
        "    non_max_suppression as nms\n",
        ")"
      ],
      "metadata": {
        "id": "MaiAHNB828NY",
        "collapsed": true
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "metadata": {
        "id": "t_lutWBT58Qo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLODataset(Dataset):\n",
        "  def __init__(\n",
        "      self,\n",
        "      csv_file,\n",
        "      img_dir, label_dir,\n",
        "      anchors,\n",
        "      image_size=416,\n",
        "      S=[13, 26, 52],\n",
        "      C=20,\n",
        "      transform=None\n",
        "  ):\n",
        "    self.annotations = pd.read_csv(csv_file)\n",
        "    self.img_dir = img_dir\n",
        "    self.label_dir = label_dir\n",
        "    self.transform = transform\n",
        "    self.S = S\n",
        "    self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])\n",
        "    self.num_anchors = self.anchors.shape[0]\n",
        "    self.num_anchors_per_scale = self.num_anchors // 3\n",
        "    self.C = C\n",
        "    self.ignore_iou_thresh = 0.5\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
        "    bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndim=2), 4, axis=1).tolist()\n",
        "    img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
        "    image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "\n",
        "    if self.transform:\n",
        "      augmentations = self.transform(image=image, bboxes=bboxes)\n",
        "      image = augmentations[\"image\"]\n",
        "      bboxes = augmentations[\"bboxes\"]\n",
        "\n",
        "    targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S]\n",
        "\n",
        "    for box in bboxes:\n",
        "      iou_anchors = iou(torch.tensor(box[2:4]), self.anchors)\n",
        "      anchor_indicies = iou_anchors.argsort(descending=True, dim =0)\n",
        "      x, Y, width, height, class_label = box\n",
        "      has_anchor = [False, False, False]\n",
        "\n",
        "      for anchor_idx in anchor_indicies:\n",
        "        scale_idx = anchor_idx // self.num_anchors_per_scale\n",
        "        anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
        "        S = self.S[scale_idx]\n",
        "        i, j = int(S * Y), int(S * x)\n",
        "        anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
        "\n",
        "        if not anchor_taken and not has_anchor[scale_idx]:\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
        "          x_cell, y_cell = S * x - j\n",
        "          width_cell, height_cell = (\n",
        "              width * S,\n",
        "              height * S\n",
        "          )\n",
        "          box_coordinates = torch.tensor(\n",
        "              [x_cell, y_cell, width_cell, height_cell]\n",
        "          )\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
        "          has_anchor[scale_idx] = True\n",
        "\n",
        "        elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
        "          targets[scale_idx][anchor_on_scale, i, j, 0] = -1\n",
        "\n",
        "    return image, tuple(targets)"
      ],
      "metadata": {
        "id": "d1OhyWiDIqZJ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Implementation"
      ],
      "metadata": {
        "id": "EHvAjMjEP29v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import intersection_over_union"
      ],
      "metadata": {
        "id": "bNAD8alXPyQk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLOLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.mse = nn.MSELoss()\n",
        "    self.bce = nn.BCEWithLogitsLoss()\n",
        "    self.entropy = nn.CrossEntropyLoss()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    self.lambda_class = 1\n",
        "    self.lambda_noobj = 10\n",
        "    self.lambda_obj = 1\n",
        "    self.lambda_box = 10\n",
        "\n",
        "  def forward(self, prediction, target, anchors):\n",
        "    obj = target[..., 0] == 1\n",
        "    noobj = target[..., 0] == 0\n",
        "\n",
        "    # No object loss\n",
        "    no_object_loss = self.bce(\n",
        "        (prediction[..., 0:1][noobj]), (target[..., 0:1][noobj])\n",
        "    )\n",
        "\n",
        "    # Object loss\n",
        "    anchors = anchors.reshape(1, 3, 1, 1, 2)\n",
        "    box_preds = torch.cat([self.sigmoid(prediction[..., 1:3]), torch.exp(prediction[..., 3:5] * anchors)], dim=1)\n",
        "    ious = intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n",
        "    object_loss = self.bce((prediction[..., 0:1][obj]), (ious * target[..., 0:1][obj]))\n",
        "\n",
        "    #Box Coordinate loss\n",
        "    prediction[..., 1:3] = self.sigmoid(prediction[..., 1:3])\n",
        "    target[..., 3:5] = torch.log(\n",
        "        (1e-16 + target[..., 3:5] / anchors)\n",
        "    )\n",
        "    box_loss = self.mse(prediction[..., 1:5][obj], target[..., 1:5][obj])\n",
        "\n",
        "    #Class loss\n",
        "    class_loss = self.entropy(\n",
        "        (prediction[..., 5:][obj]), (target[..., 5][obj].long())\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        self.lambda_box * box_loss\n",
        "        + self.lambda_obj * object_loss\n",
        "        + self.lambda_noobj * no_object_loss\n",
        "        + self.lambda_class * class_loss\n",
        "    )"
      ],
      "metadata": {
        "id": "RdXinV93P_Cl"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "7enmDhCSW20y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from utils import (\n",
        "    mean_average_precision,\n",
        "    cells_to_bboxes,\n",
        "    get_evaluation_bboxes,\n",
        "    save_checkpoint,\n",
        "    load_checkpoint,\n",
        "    check_class_accuracy,\n",
        "    get_loaders,\n",
        "    plot_couple_examples\n",
        ")"
      ],
      "metadata": {
        "id": "1tXlTCK3W2WZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "GVBqdpaaYIcQ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
        "  loop = tqdm(train_loader, leave=True)\n",
        "  losses = []\n",
        "\n",
        "  for batch_idx, (x, y) in enumerate(loop):\n",
        "    x = x.to(config.DEVICE)\n",
        "    y0, y1, y2 = (\n",
        "        y[0].to(config.DEVICE),\n",
        "        y[1].to(config.DEVICE),\n",
        "        y[2].to(config.DEVICE)\n",
        "    )\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "      out = model(x)\n",
        "      loss = (\n",
        "          loss_fn(out[0], y0, scaled_anchors[0])\n",
        "          + loss_fn(out[1], y1, scaled_anchors[1])\n",
        "          + loss_fn(out[2], y2, scaled_anchors[2])\n",
        "      )\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    mean_loss = sum(losses) / len(losses)\n",
        "    loop.set_postfix(loss=mean_loss)"
      ],
      "metadata": {
        "id": "1NSTnlBdYMu1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  model = YOLOv3(num_classes=config.NUM_CLASSES).to(config.DEVICE)\n",
        "  optimizer = optim.Adam(\n",
        "      model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY\n",
        "  )\n",
        "  loss_fn = YOLOLoss()\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "  train_loader, test_loader, train_eval_loader = get_loaders(\n",
        "      train_csv_path=config.DATASET + \"/100examples.csv\",\n",
        "      test_csv_path=config.DATASET + \"/100examples.csv\",\n",
        "  )\n",
        "\n",
        "  if config.LOAD_MODEL:\n",
        "    load_checkpoint(\n",
        "        config.CHECKPOINT_FILE, model, optimizer, config.LEARNING_RATE\n",
        "    )\n",
        "\n",
        "  scaled_anchors = (\n",
        "      torch.tensor(config.ANCHORS)\n",
        "      * torch.tensor(config.S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "  ).to(config.DEVICE)\n",
        "\n",
        "  for epoch in range(config.NUM_EPOCHS):\n",
        "    train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
        "\n",
        "    if config.SAVE_MODEL:\n",
        "      save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
        "\n",
        "    if epoch % 10 == 0 and epoch > 0:\n",
        "      print(\"On Test loader:\")\n",
        "      check_class_accuracy(model, test_loader, threshold=config.CONF_THRESHOLD)\n",
        "\n",
        "      pred_boxes, true_boxes = get_evaluation_bboxes(\n",
        "          test_loader,\n",
        "          model,\n",
        "          iou_threshold=config.NMS_IOU_THRESH,\n",
        "          anchors=config.ANCHORS,\n",
        "          threshold=config.CONF_THRESHOLD,\n",
        "      )\n",
        "\n",
        "      mapval = mean_average_precision(\n",
        "          pred_boxes,\n",
        "          true_boxes,\n",
        "          iou_threshold=config.MAP_IOU_THRESH,\n",
        "          box_format=\"midpoint\",\n",
        "          num_classes=config.NUM_CLASSES,\n",
        "      )\n",
        "      print(f\"MAP: {mapval.item()}\")"
      ],
      "metadata": {
        "id": "BVhBpLtxYg-r"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test on Pascal Voc"
      ],
      "metadata": {
        "id": "LG2pbVZ-cWkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"aladdinpersson/pascal-voc-dataset-used-in-yolov3-video\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByTv_XAAcVQ3",
        "outputId": "f8904d14-936e-4797-9b50-05d5f540a515"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/aladdinpersson/pascal-voc-dataset-used-in-yolov3-video?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.31G/4.31G [01:12<00:00, 63.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/aladdinpersson/pascal-voc-dataset-used-in-yolov3-video/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /root/.cache/kagglehub/datasets/aladdinpersson/pascal-voc-dataset-used-in-yolov3-video/versions/1 /content/pascal"
      ],
      "metadata": {
        "id": "oNmnYiuLdKCW"
      },
      "execution_count": 49,
      "outputs": []
    }
  ]
}